{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Making sure satellite algorithms meet people's needs","text":"<p>Information from the ground is necessary to design models that meet peoples needs.  These materials will guide you through an example of evaluating and improving a satellite index of drought.</p>"},{"location":"#client-input-is-necessary-for-design","title":"Client input is necessary for design","text":"<p>There are many things that the human or AI engineer does not know about the client's needs. For example, rainfall alone does not determine when a farmer can sow, or needs coverage to start. It is necessary to know what flexibility farmers have in crop types and labor availability to answer this question.</p> <ul> <li>Makes products that are better suited to clientelle</li> <li>Improves demand. Farmers who have had input on their products have more ownership and understanding.</li> <li>Reduces complaints. Products are more transparent, and many errors are discovered and addressed before failure happens.</li> <li>Improves accuracy. We have found that farmers themselves nearly universally design better indexes than provided by experts.</li> <li>Provides clientelle with voice and agency</li> </ul>"},{"location":"comparisiontable/","title":"Evaluating information and indexes","text":"<p>The chart below compares the bad years that the village reported with the years that had low rainfall in the satellite record, during the highlighted times just discussed.</p> <ul> <li>Years are listed on the x axis</li> <li>Taller bars are worse years, </li> <li>Grey bars are the farmer reported droughts</li> <li>The Tallest grey bar is the worst year the village remembered</li> </ul> <p>The satellite rainfall record is filtered by the farmer reported vulnerability timing</p> <ul> <li> <p>The early season sowing (blue), late season flowering (orange), and vegetative de-greening (green) periods are the same colors as before</p> </li> <li> <p>the colored bars represent how bad the satellite rainfall was during those decads in each year</p> </li> <li> <p>There is an additional red bar that represents if any of the satellite metrics (the combined severity) reflected a bad year, which was used to design the insurance index.</p> </li> <li> <p>A red o marks any year where none of the satellite indicators were bad enough to be considered a meaningful deficit.</p> </li> </ul> <p>You can explore the agreement in the information </p> <ul> <li> <p>Which years did the farmers say were bad that were also represented in the satelite data for the reported seasonal timing?</p> </li> <li> <p>Which farmer bad years were not reflected in the satellite data with that timing?</p> </li> <li> <p>What is the ratio of years with hits vs misses?</p> </li> </ul> <p>The answers to those questions are summarized automatically if you scroll to the matching table at the bottom of the page.</p> <p></p> <p></p> <p>In the matching table, you can see how much any data source matched another by looking at the rows and colums.  For example, you can see that the combined severity agreed with the farmer bad years 50% of the time.</p> <p>But what if we wanted to know the matching of the timing for rainfall vulnerability earlier in the season?</p>"},{"location":"comparisonexample/","title":"Reported Rainfall timing in Village of Genete (Ethiopia)","text":"<p>In the village of Genete, we can perform this crosscheck.</p> <p>We will use the timing of the season in decads, a common form for agronomists.  A decad reflects approximately 10 days from the new year, so decad 4 starts Feb 1, Decad 7 starts March 1, Decad 19 July 1, Decad 25 is September 1.</p> <p>The quantitative focus group in Genete said that the key times they were vulnerable to rainfall were during flowering (late July and September) and preparation and planting, beginning in March and continuing into early July.</p> <p>Below you can see the Rainfall climatology for the village, is the average rainfall during the year over their reported cropping cycle. The most vulnerable times of year initially reported are higlighted in blue for the beginning of the year, and orange for the flowering, later in the year.  </p> <p>To provide ain dditional verification source, green bar reflects the timing that the vegetation  would change color to reflect seasonal end, after the rainfall in the time highlighted in orange had passed, and the landscape had time to respond.</p> <p>There was intense discussion about if the most important start of the season was decad 7 or 18.</p> <p></p>"},{"location":"comparisonexample/#evaluating-information-and-indexes","title":"Evaluating information and indexes","text":"<p>The chart below compares the bad years that the village reported with the years that had low rainfall in the satellite record, during the highlighted times just discussed.</p> <ul> <li>Years are listed on the x axis</li> <li>Taller bars are worse years, </li> <li>Grey bars are the farmer reported droughts</li> <li>The Tallest grey bar is the worst year the village remembered</li> </ul> <p>The satellite rainfall record is filtered by the farmer reported vulnerability timing</p> <ul> <li> <p>The early season sowing (blue), late season flowering (orange), and vegetative de-greening (green) periods are the same colors as before</p> </li> <li> <p>the colored bars represent how bad the satellite rainfall was during those decads in each year</p> </li> <li> <p>There is an additional red bar that represents if any of the satellite metrics (the combined severity) reflected a bad year, which was used to design the insurance index.</p> </li> <li> <p>A red o marks any year where none of the satellite indicators were bad enough to be considered a meaningful deficit.</p> </li> </ul> <p>You can explore the agreement in the information </p> <ul> <li> <p>Which years did the farmers say were bad that were also represented in the satelite data for the reported seasonal timing?</p> </li> <li> <p>Which farmer bad years were not reflected in the satellite data with that timing?</p> </li> <li> <p>What is the ratio of years with hits vs misses?</p> </li> </ul> <p>The answers to those questions are summarized automatically if you scroll to the matching table at the bottom of the page.</p> <p></p> <p></p> <p>In the matching table, you can see how much any data source matched another by looking at the rows and colums.  For example, you can see that the combined severity agreed with the farmer bad years 50% of the time.</p> <p>But what if we wanted to know the matching of the timing for rainfall vulnerability earlier in the season?</p>"},{"location":"concreteinput/","title":"Concrete input","text":"<p>In this example we utilize these two questions for concrete, specific input.</p> <ol> <li>What specific times of year are you most vulnerable to drought?</li> <li>What were the most severe drought years you have experienced?    </li> </ol> <p>The drought years are ordered in severity by the community, with 1 being the worst year.</p> <p>For strategies that have been low cost and scalable in effectively framing these questions to communities,  you can refer to Project Field Guides</p>"},{"location":"concreteinput/#crosscheck-example","title":"Crosscheck Example","text":"<p>For our example, this has proven to be an effective crosscheck using satellite data and focus group input.</p> <ol> <li>Check the satellite rainfall data during the times of the season for the lowest rainfall years</li> <li>Compare against the list of years from the focus groups</li> </ol> <p>This provides a simultaneous statistical test of the accuracy of</p> <ol> <li>focus group seasonal timing reporting</li> <li>focus group drought severity ranking</li> <li>satellite rainfall data</li> </ol> <p>If the drought years agree between the satellite data focused on reported seasonal timing  and the drought years remembered, then that is a statistical test demonstrating accuracy of all datasets.</p> <p>If the datasets do not agree, we do not know which are inaccurate. </p> <p>But we can explore timing parameters to optimize fit using various statistical and performance metrics.</p>"},{"location":"crosscheck/","title":"Crosscheck Example","text":"<p>For our example, this has proven to be an effective crosscheck using satellite data and focus group input.</p> <ol> <li>Check the satellite rainfall data during the times of the season for the lowest rainfall years</li> <li>Compare against the list of years from the focus groups</li> </ol> <p>This provides a simultaneous statistical test of the accuracy of</p> <ol> <li>focus group seasonal timing reporting</li> <li>focus group drought severity ranking</li> <li>satellite rainfall data</li> </ol> <p>If the drought years agree between the satellite data focused on reported seasonal timing  and the drought years remembered, then that is a statistical test demonstrating accuracy of all datasets.</p> <p>If the datasets do not agree, we do not know which are inaccurate. </p> <p>But we can explore timing parameters to optimize fit using various statistical and performance metrics.</p>"},{"location":"feedback/","title":"Answers from Phone Feedback","text":"<p>The answers from the people who have completed the phone feedback \"to date\" (the day we wrote this part) can help us think through what to do next in the index design.</p> <p>**Remember that the phone feedback is from folks going through this exercise, so it is not from the actual people who would be benefitting from the product.  This is only a training example to show how this information could be used! **</p> <p>But lets pretend the answers came from farmers, and community representatives...</p> <p>The average answers were:</p> <ul> <li>2002 50%</li> <li>1984 50%</li> </ul> <p>This illustrates that design is never easy!  Because were evenly split between wanting payouts in 2002 and 1984, it is still hard to make a choice, but could suggest that two different insurance products  are what the community really wants.</p> <p>But, we can dig deeper.  We can check the test question. If we assume that people who answered a test question \"correctly\" are giving more accurate answers, we can filter by that.  For our test question of which year was worse, 2009 or 2010 we believe 2009 is the correct answer.  </p> <p>The respondents answered:</p> <ul> <li>2009 62.5%</li> <li>2010 37.5%</li> </ul> <p>This means most people agreed with our belief.  We can use this to select people who we believe are answering more accurately.  We therefore do another attempt, where we calculate the responses of only the people who answered the test question \"correctly\".</p> <p>Looking only at the people who answered the test question \"correctly,\" we now have new results:</p> <ul> <li>2002 80%</li> <li>1984 20%</li> </ul> <p>We could therefore select the design that focuses on the decads 10-17, based on the pretend farmer feedback from the participants of this illustration.  </p> <p>Ironically, this is not so different from what the actual farmers themselves said when asked the same question (in person, because it was a few years ago).  In the actual insurance project, many farmers wanted an index that was like the one in decads 6-17, because they wanted to de-risk loans for long cycle crops.  The choice of early season vs later season start generally depended on the village, and was closely driven by how big the early seasonal \"bump\" in rainfall was. Across all of the villages polled that year, 57% chose the window that covered the later start, and 43% chose the earlier start window.  </p> <p>Having farmers provide low cost ongoing input like this, iterating over the years their situations change that leads to meaningful impacts and sustained demand, for these villages over more than a decade.</p>"},{"location":"key/","title":"Key","text":""},{"location":"key/#unlocking-large-scale-feedback","title":"Unlocking large scale feedback","text":"<p>Information from large scale co-design can include a lot of noise. It is necessary to have a strategy that can filter the truth from the noise.</p>"},{"location":"key/#key-components","title":"Key components","text":"<p>We need two components for this approach to quantitative client codesign</p> <ol> <li>Concrete specific input. Open discussion is important in codesign,  but for this part, it is critical to obtain engineering quality quantitative input. In the past key design choices have been incorrect because the co-design was not adequately quantitative.</li> <li>Crosschecking verification. It is important to have an evaluation mechanism embedded at the heart of the process to identify solutions and filter out noise.</li> </ol>"},{"location":"key/#real-world-performance","title":"Real world performance","text":"<p>This strategy has been successful at large scales for low cost, in thousands of villages across dozens of countries. For example, the government of Zambia performed this exercise across the entire country, with most of the site visits completed within a few weeks.</p> <p>We will work through an example for a satellite model of drought in Ethiopia, used for an insurance index.</p> <p></p>"},{"location":"noki/","title":"Phone feedback, games, and contests","text":"<p>One way to obtain fast feedback is to utilize two way phone text or recorded voice communications. We have found that providing incentives for accuracy, such as awards, or games can be important.</p> <p>However, these kinds of communications can yield noisy data, often with inaccurate or strategic responses. </p> <p>Research has found that two strategies can be effective in filtering information</p> <ol> <li>Incentivize accuracy: provide a prize or honor for being more accurate</li> <li>Verification questions: ask questions we know the answer to, and focus on responses of people who got those questions \"right\" </li> </ol> <p>Using the flexible NOKI platform in the DESDR toolkit we could set up this contest to quickly incentivize cogeneration feedback.</p> <p>We can frame the question as a contest--how well could a farmer playing from a phone represent the preferences of their community over which kind of year is more important, 1984, or 2002. </p> <p>The player could win (and perhaps get a prize) if they can guess what others would say.</p> <p>We also know that 2009 was consistently bad, and 2010 was consistently good, so we could also ask people to compare those years so to help filter results.</p> <p>To experience an example of a phone codesign implimentation that you might use, text or whatsapp Genete to +1 (646) 217-0881</p> <p>These tools are flexible for different cogeneration challenges, and they are open source. What would you use these tools and approaches for?</p> <p></p>"},{"location":"slideyouself/","title":"Design improvement","text":"<p>You can explore the farmers debate of if rainfall starting around decad 7 is more related to vulnerability than rainfall starting in decad 18.</p> <p>The below is an interactive tool to evaluate and co-design indexes.  It is part of our public domain DESDR open toolkit. </p> <p>It has all of the tables and figures we have just worked through but it also has controls</p> <p>Click on the tool below, then drag the sliders for the rainfall early timing to be more focused on the rainfall starting around decad 7 and extending 5-10 decads.</p> <p>Do you see changes in matching?  </p> <p>Do you have a new timing to verify with farmers?</p> <p>Click on the tool below to explore these </p>"},{"location":"slideyouselfClicktoopen/","title":"Design improvement","text":"<p>You can explore the farmers debate of if rainfall starting around decad 7 is more related to vulnerability than rainfall starting in decad 18.</p> <p>The below is an interactive tool to evaluate and co-design indexes.  It is part of our public domain DESDR open toolkit. </p> <p>It has all of the tables and figures we have just worked through but it also has controls</p> <p>Click on the tool below, then drag the sliders for the rainfall early timing to be more focused on the rainfall starting around decad 7 and extending 5-10 decads.</p> <p>Do you see changes in matching?  </p> <p>Do you have a new timing to verify with farmers?</p> <p>Click on the tool below to explore these </p> <p></p>"},{"location":"toughchoices/","title":"Not enough information","text":"<p>Although years like 2009 is consistently bad, and 2010 is consistently good...</p> <p>Using the design and evaluation tools we can see that there are multiple options with the same level of high agreement. For example.</p> <p>The very early time period from decads 6-17:</p> <ul> <li>88% agreement</li> <li>Reflects the challenges in 1984 </li> <li>Doesnt reflect 2002, and has very small anomalies (potentially small insurance payouts) in years like 1994</li> </ul> <p>The slightly later time period from decads 10-17:</p> <ul> <li>88% agreement</li> <li>Reflects 2002 and 1994 </li> <li>Doesnt reflect 1984</li> </ul> <p>This is because the seasonal timing of rainfall in 2002 was later than that of 1984.  </p>"},{"location":"toughchoices/#what-would-you-do","title":"What would you do?","text":"<p>Explore the early window timing and say what looks best to you below.  If you need help, check the hint just below the form.</p> Loading\u2026 <p>Hint: Dont feel like you have enough information to make these choices? Thats the point of this exercise!  Without client cogeneration inside of the design loop, we often do not have good ways to make these choices.  </p> <p>If only we could ask the farmers if they would prefer to have coverage that focused on rainfall as early in the season as the deficit of 1984 or if they were able to shift their practices in response to the rainfall timing to want coverage that focused on the later timing of 2002.  Can you guess what we are leading up to?</p> <p>Next we will explore a cogeneration tool that can address that challenge.</p>"}]}